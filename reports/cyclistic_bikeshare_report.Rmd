---
title: "Cyclistic bike-share analysis"
author: "Paidraig 'Patrick' O'Ceallaigh"
date: "2024-04-24"
output: 
  html_document:
    css: "../resources/css/style.css"
    df_print: paged
    theme: sandstone
    highlight: tango
    toc: true
    toc_depth: 4
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  fig.align = "center",
  fig.width = 6,
  fig.asp = 0.618
)

library(tidyverse)
library(here)

source(here("R", "01_load_data.R"))
source(here("R", "02_inspect_data.R"))
source(here("R", "03_transform_data.R"))
source(here("R", "04_analyze_data.R"))
source(here("R", "style_tables.R"))
```

## Executive summary    


## Introduction    

In the evolving landscape of urban mobility, bike-share programs have emerged as a cornerstone for convenient and sustainable transportation. Cyclistic, a prominent bike-share company based in Chicago, has carved a niche in this dynamic market since its inception in 2016. Featuring a diverse fleet of over 5,800 bicycles, including specialized models like reclining bikes and hand tricycles, Cyclistic ensures inclusivity and accessibility for a broad user base. The program's infrastructure is robust, with 692 docking stations across Chicago that support geotracking and seamless bike exchanges. While the company's diverse fleet and flexible pricing plans have been instrumental in attracting a wide range of users, the majority of rides are utilized for leisure, with approximately 30% serving commuters.

### Background    

Under the guidance of Lily Moreno, the Director of Marketing, Cyclistic has achieved significant growth through strategic marketing and customer engagement initiatives. Historically, the company's marketing efforts have been geared towards building brand awareness and appealing to a broad consumer base, utilizing channels like email, social media, and direct promotions. Cyclistic offers a range of pricing options from single-ride passes to annual memberships, with the latter being identified as considerably more profitable. This insight has prompted a strategic pivot aimed at increasing the proportion of annual memberships, which are deemed crucial for the company's long-term financial health and market dominance.

Despite the success, Cyclistic's executive team, known for their meticulous attention to detail, requires a data-driven approach to further refine and optimize marketing strategies. This is where the marketing analytics team, including junior data analysts like yourself, plays a pivotal role. Having joined the team six months ago, your objective has been to not only grasp Cyclistic's operational ethos but also to contribute to its mission through rigorous data analysis and actionable insights.

### Objectives    

The primary objective of this analysis is to delineate the usage patterns of annual members versus casual riders. By understanding these differences, the team aims to uncover the underlying factors that influence a rider's decision to opt for casual passes or commit to an annual membership. This investigation will focus on various aspects such as duration of rides, preferred bike types, peak usage times, and the purposes of trips (leisure versus commuting).

The insights derived from this analysis will serve as the foundation for developing targeted marketing strategies aimed at converting casual riders into annual members. This strategic shift is not only expected to enhance customer retention but also to maximize profitability. The upcoming recommendations will need to be backed by compelling data visualizations and insights that can persuade the executive team of their viability and potential impact.

In summary, this analysis is not just about understanding how two segments of riders utilize the Cyclistic program, but about leveraging this understanding to foster a more sustainable growth trajectory through increased membership conversion.

### Research question    

> How do annual members and casual riders use Cyclisitc bikes differently?

## Data collection    

### Data sources   

For the comprehensive analysis of usage patterns between annual members and casual riders, the primary data source comprises the most recent twelve months of historical trip data from Cyclistic's bike-share program. This dataset is publicly accessible and consists of twelve individual files, each representing a month's worth of data, formatted in CSV. These files encapsulate extensive records of all bike trips taken during this period, capturing a variety of attributes related to each trip. The data has been made available under the terms specified in the provided [license](https://divvybikes.com/data-license-agreement) and can be downloaded [here](https://divvy-tripdata.s3.amazonaws.com/index.html).

```{r raw_data, warning=FALSE, message=FALSE, echo=FALSE}
# Load the raw dataset image and float it to the center
htmltools::img(
  src = here("resources", "images", "raw_data.png"),
  align = "center",
  alt = "Cyclistic dataset: 04/2023 - 03/2024",
  style = "width: 60%"
)
```

### Data description  

The raw dataset is structured with each row representing a unique bike trip, characterized by the following variables and their respective types:

```{r raw_dataset_table, warning=FALSE, message=FALSE, echo=FALSE}
# Create a tibble for variable information
variable_name <- c(
  "`ride_id`", "`rideable_type`",
  "`started_at`", "`ended_at`",
  "`start_station_name`", "`start_station_id`",
  "`end_station_name`", "`end_station_id`",
  "`start_lat`", "`start_lng`",
  "`end_lat`", "`end_lng`",
  "`member_casual`"
)

variable_description <- c(
  "A unique id for each bike ride",
  "Type of bicyle used for the ride",
  "Timestamp for the begining of the ride",
  "Timestamp for the end of the ride",
  "Name of the station from which the ride started ",
  "ID of the staion from which the ride started",
  "Name of the station at which the ride ended",
  "ID of the station at which the ride ended",
  "Latitude at which the ride started",
  "Longitude at which the ride started",
  "Latitude at which the ride ended",
  "Longitude at which the ride ended",
  "Type of rider"
)

variable_type <- c(
  "<chr>", "<chr>", "<dttm>", "<dttm>",
  "<chr>", "<chr>", "<chr>", "<chr>", "<dbl>",
  "<dbl>", "<dbl>", "<dbl>", "<chr>"
)

variable_info <- tibble(variable_name, variable_type, variable_description)

variable_info %>%
  kable(
    caption = "Variables",
    col.names = c("Variable", "Type", "Description")
  ) %>%
  custom_table_theme()
```

This detailed and structured dataset provides a solid foundation for analyzing and comparing the different usage patterns of Cyclistic's bike-share system by annual members and casual riders. Through this analysis, the marketing analytics team aims to derive insights that can influence strategic marketing decisions, ultimately converting more casual riders into annual members.

### Challenges and limitations    

While the dataset is rich in information and provides a comprehensive view of bike trips taken over the past year, there are several challenges and limitations that need to be considered:

1. **Data Volume and Processing**
    - **Challenge**: Combining data from 12 CSV files with over 5,000,000 rows can be computationally intensive and time-consuming.  
    - **Mitigation**: Utilize efficient data manipulation packages in R such as `dtplyr` "lazy" data.table for its superior performance with large datasets and `dplyr` for its intuitive syntax and data manipulation capabilities.
2. **Joining and Merging Data**
    - **Challenge**: Merging data from multiple sources into a cohesive dataset requires careful data handling to ensure integrity.
    - **Mitigation**: Use `vroom` for fast reading of CSV files and `dtplyr` and `dplyr` for efficient data manipulation and merging.
3. **Data Cleaning and Preprocessing**
    - **Challenge**: The raw data may contain missing values, outliers, or inconsistencies that need to be addressed before analysis.
    - **Mitigation**: Use R functions (e.g., `dplyr`, `tidyr`) for data cleaning (handling missing values, duplicates, outliers).
4. **Geospatial Operations**
    - **Challenge**: Handling latitude and longitude data introduces specific challenges related to spatial data accuracy and consistency.
        - Calculating distances between coordinates results in distances that are "as the crow flies" and may not reflect actual travel distances.
    - **Mitigation**: Utilize geospatial packages like `geosphere`, sf`, and `leaflet` for geospatial analysis and visualization.
5. **Data Privacy and Ethical Considerations**
    - **Challenge**: Maintaining confidentiality and adhering to ethical data use standards is essential.
    - **Mitigation**: Anonymize sensitive data using R functions before analysis. Ensure all data use complies with ethical standards and legal requirements.
     
## Data cleaning    

### Data inspection  

The data inspection process serves as a preliminary check to ensure the quality and structure of the dataset before proceeding with detailed analysis. This stage is crucial for identifying potential issues that could affect the validity of the studyâ€™s conclusions. The following steps were undertaken to inspect the dataset comprehensively:

#### Examine structure of data     

To begin, we examined the overall structure and type of the data contained within the dataset. This was accomplished using the `glimpse()` function from the `dplyr` package, which provides a compact display of the types of each column along with a quick view of the first few entries in each:

```{r}
data_raw %>% 
  glimpse()
```


#### Check duplicate observations    

Identifying and eliminating duplicate records is essential to ensure the integrity of the analysis. Given that each record in the dataset contains a unique identifier called `ride_id`, we anticipated minimal duplication. To verify this, the dataset was processed using the `dplyr` package, which is part of the comprehensive tidyverse suite of tools for data manipulation in R. We grouped the data by all columns to account for complete record duplication, filtered out any duplicates, and counted the remaining rows:

```{r duplicate_obs, echo=TRUE, eval=FALSE}
duplicate_obs <- data_raw %>%
  group_by_all() %>%
  filter(n() > 1) %>%
  ungroup() %>%
  nrow_lazy_dt()
```

After this thorough check, no duplicate records were found in the dataset, confirming that each `ride_id` indeed represents a unique trip event. This finding solidifies the datasetâ€™s reliability for subsequent analyses, ensuring that each entry uniquely contributes to the insights generated.


#### Check missing values    

Handling missing data is crucial as it can significantly impact the analysis. To assess and quantify missing values within the dataset, we utilized the `dplyr` and `tidyr` packages from the tidyverse collection, which are robust tools for data manipulation in R. The procedure involved summarizing the missing values across all columns and transforming the results for a clearer presentation:     

```{r missing_values, echo=TRUE, eval=FALSE}
missing_obs <- data_raw %>%
  summarise(across(everything(), ~ sum(is.na(.), na.rm = TRUE))) %>%
  pivot_longer(
    everything(),
    names_to = "variable",
    values_to = "missing_count"
  ) %>%
  filter(missing_count > 0) %>%
  as_tibble()
```

Upon inspection, it was discovered that certain variables related to station identification and location data exhibited notable gaps:

```{r missing_values_table, warning=FALSE, message=FALSE, echo=FALSE}
missing_values <- c(0, 0, 0, 0, 874450, 874450, 929226, 929226, 0, 0, 7566, 7566, 0)

missing_info <- tibble(variable_name, missing_values)

# Display only the variables with missing data
missing_info %>%
  filter(
    missing_values > 0
  ) %>%
  kable(
    caption = "Missing values",
    col.names = c("Variable", "Missing values")
  ) %>%
  custom_table_theme()
```


These findings indicate significant missing data in station identifiers and geolocation details, which could affect spatial analysis and ride linkage accuracy. Such gaps will necessitate careful consideration in subsequent stages of data cleaning and analysis to ensure that conclusions drawn are based on comprehensive and accurate information.

#### Check categorical variables    

Analyzing the consistency and accuracy of categorical variables is essential for ensuring data integrity. In this analysis, the `dplyr` package was employed to count and evaluate the distinct categories present in the `rideable_type` and `member_casual` variables.

##### Rideable_type variable     
The `rideable_type` variable categorizes the types of bikes used in the trips. While only `classic_bike` and `electric_bike` were expected based on company records, an additional category, `docked bike`, was identified, which was not anticipated:

```{r count_rideable_type, echo=TRUE, eval=FALSE}
count_rideable_type <- data_raw %>%
  count_categorical(rideable_type)
```

The presence of `docked bike` raises questions about its meaning, as it could potentially refer to bikes that are either not clearly classified between the two expected types or perhaps represent bikes that have been temporarily removed from active service for maintenance. This ambiguity requires further investigation to accurately interpret the data involving this category.

##### Member_casual variable     

member_casual: For the member_casual variable, which distinguishes between members (annual subscribers) and casual riders (non-subscribers), the results were as expected. Only the two anticipated values, "member" and "casual," were found, confirming the accuracy of data entries for this variable:

```{r count_member_casual, echo=TRUE, eval=FALSE}
count_member_casual <- data_raw %>%
  count_categorical(member_casual)
```

This assessment underscores the importance of validating categorical data against expected outcomes to ensure the reliability of analyses based on these classifications. For the `rideable_type` variable, particularly, the unexpected category will be noted for potential adjustments in the analysis framework or for clarification from data management teams.

### Data transformation     

The data transformation process involves several steps aimed at refining the dataset to enhance the quality and relevance of the analysis. These transformations are crucial for ensuring that the data is clean, properly structured, and ready for detailed analytical tasks. The following transformations were performed on the Cyclistic dataset:

#### Remove unnecessary variables     

Initially, the dataset included variables related to station identification, specifically the names and IDs of start and end stations (start_station_name, start_station_id, end_station_name, end_station_id). These variables comprised the majority of missing data within the dataset, posing potential challenges for complete and accurate analysis. While station identification information could offer valuable insights for future studies, particularly in understanding spatial patterns and usage trends, it was not pertinent to the current analysis's objectives. Consequently, to streamline the dataset and focus on the most relevant variables for examining user behavior and bike usage patterns, these station identification variables were removed:

```{r remove_irrelevant_vars, echo=TRUE, eval=FALSE}
irrelevant_vars <- c(
  "start_station_name",
  "start_station_id",
  "end_station_name",
  "end_station_id"
)

data_processed <- data_raw %>%
  select(-all_of(irrelevant_vars))
```

This step not only reduced the complexity of the data but also minimized the impact of the missing data on the overall analysis, ensuring a more focused and efficient analytical process.

#### Remove missing values

The presence of missing values can hinder the accuracy and reliability of the analysis. To address this issue, the dataset was filtered to exclude rows with missing values in the latitude and longitude columns for both the start and end stations. This step was crucial for ensuring the integrity of geospatial analyses and visualizations that rely on accurate location data:     

```{r remove_missing_values, echo=TRUE, eval=FALSE}
data_processed <- data_processed %>%
  filter(
    !is.na(end_lat),
    !is.na(end_lng)
  )

obs_nonmissing <- data_processed %>%
  nrow_lazy_dt()
```



